# BERT 토크나이저와 RoBERTa 토크나이저

from transformers import AutoTokenizer



bert_tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')
'''
{
  'input_ids': [
    [2, 1656, 1141, 3135, 6265, 3], 
    [2, 864, 1141, 3135, 6265, 2073, 1656, 1141, 3135, 6265, 2178, 2062, 831, 647, 2062, 3]
  ], 
  'token_type_ids': [
    [0, 0, 0, 0, 0, 0], 
    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
  ], 
  'attention_mask': [
    [1, 1, 1, 1, 1, 1], 
    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
  ]
}
'''
print(bert_tokenizer(['첫 번째 문장', '두 번째 문장은 첫 번째 문장보다 더 길다']))

'''
{
  'input_ids': [
    [2, 1656, 1141, 3135, 6265, 3], 
    [2, 864, 1141, 3135, 6265, 2073, 1656, 1141, 3135, 6265, 2178, 2062, 831, 647, 2062, 3]
  ], 
  'token_type_ids': [
    [0, 0, 0, 0, 0, 0], 
    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
  ], 
  'attention_mask': [
    [1, 1, 1, 1, 1, 1], 
    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
  ]
}
'''

'''
attension_mask 값이 0인 것은 padding 값
{
  'input_ids': [
    [2, 1656, 1141, 3135, 6265, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
    [2, 864, 1141, 3135, 6265, 2073, 1656, 1141, 3135, 6265, 2178, 2062, 831, 647, 2062, 3]
  ], 
  'token_type_ids': [
    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
  ], 
  'attention_mask': [
    [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
  ]
}
'''
print(bert_tokenizer(['첫 번째 문장', '두 번째 문장은 첫 번째 문장보다 더 길다'], padding='longest'))


